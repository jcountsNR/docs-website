---
title: Collector for Confluent Cloud & Kafka monitoring
tags:
  - Integrations
  - Open source telemetry integrations
  - OpenTelemetry
  - Kafka
  - Confluent Cloud
metaDescription: You can collect Kafka metrics from Confluent Cloud using the OpenTelemetry Collector.
---
You can collect metrics about your Confluent Cloud-managed Kafka deployment with the OpenTelemetry Collector. The collector is a component of OpenTelemetry that collects, processes, and exports telemetry data to New Relic (or any observability backend).

This integration works by running a prometheus receiver configuration inside the OpenTelemetry collector, which scrapes [Confluent Cloud's metrics API](https://api.telemetry.confluent.cloud/docs/descriptors/datasets/cloud?_ga=2.183807142.1264186867.1705940186-6520871.1686857317&_gl=1*1te8jue*_ga*NjUyMDg3MS4xNjg2ODU3MzE3*_ga_D2D3EGKSGD*MTcwNjAzNzYwOS41Ni4wLjE3MDYwMzc2MDkuNjAuMC4w) and exports that data to New Relic.

Complete the steps below to run the example and export the metrics to New Relic.

## Step 1: Sign up for New Relic! [#signup]

* If you haven't already done so, sign up for a free [New Relic account](https://newrelic.com/signup).
* Get the <InlinePopover type="licenseKey" />for the New Relic account to which you want to report data.

## Step 2: Prerequisites [#prerequisites]

* You must have a docker daemon running
* You must have [Docker Compose](https://docs.docker.com/compose/) installed
* You must have a [Confluent Cloud API key & secret](https://docs.confluent.io/confluent-cli/current/command-reference/api-key/confluent_api-key_create.html). Note that this is **Not** the same as a Cluster API Key in Confluent.

## Step 3: Download or clone the examples repo [#example]

This setup uses the example collector configuration in [New Relic's OpenTelemetry Examples repo](https://github.com/newrelic/newrelic-opentelemetry-examples).

Download the repo above, and navigate to the [Confluent Cloud example](https://github.com/newrelic/newrelic-opentelemetry-examples/tree/main/other-examples/collector/confluentcloud) directory. For more information, you can check the `README` there as well.

## Step 4: Set environment variables and run the collector [#configure-opentelemetry-collectors]

* Set the API key & Secret variables for both Confluent Cloud and New relic in the `.env` file
* Set the `Cluster_ID` variable with the target kafka cluster ID
* (Optional) To monitor connectors or schema registry's managed by Confluent Cloud, you can un-comment the configuration in the `collector.yaml` file and set the corresponding ID in the `.env` file

```bash
cd ./other-examples/collector/confluentcloud

docker compose up
```

### Local Variable information

    <table>
      <thead>
        <tr>
          <th style={{ width: "200px"}}>
            Variable
          </th>
          <th>
            Description
          </th>
          <th>
            Docs
          </th>
        </tr>
        </thead>
        <tbody>
          <tr>
            <td>
              `NEW_RELIC_API_KEY`
            </td>
            <td>
              New Relic Ingest API Key
            </td>
            <td>
              [API Key docs](/docs/apis/intro-apis/new-relic-api-keys/)
            </td>
          </tr>
          <tr>
            <td>
              `NEW_RELIC_OTLP_ENDPOINT`
            </td>
            <td>
              Default US New Relic OTLP endpoint is https://otlp.nr-data.net:4318
            </td>
            <td>
              [OTLP endpoint config docs](/docs/more-integrations/open-source-telemetry-integrations/opentelemetry/get-started/opentelemetry-set-up-your-app/#review-settings)
            </td>
          </tr>
          <tr>
            <td>
              `CLUSTER_ID`
            </td>
            <td>
              ID of cluster from Confluent Cloud
            </td>
            <td>
              [Docs for list cluster ID command](https://docs.confluent.io/confluent-cli/current/command-reference/kafka/cluster/confluent_kafka_cluster_list.html#description)
            </td>
          </tr>
          <tr>
            <td>
              `CONFLUENT_API_KEY`
            </td>
            <td>
              Cloud API key
            </td>
            <td>
              [Cloud API key docs](https://docs.confluent.io/cloud/current/monitoring/metrics-api.html#metrics-quick-start)
            </td>
          </tr>
          <tr>
            <td>
              `CONFLUENT_API_SECRET`
            </td>
            <td>
              Cloud API secret
            </td>
            <td>
              [Cloud API key docs](https://docs.confluent.io/cloud/current/monitoring/metrics-api.html#metrics-quick-start)
            </td>
          </tr>
          <tr>
            <td>
              `CONNECTOR_ID`
            </td>
            <td>
              (OPTIONAL) You can monitor your Confluent connectors by specifying the ID here
            </td>
            <td>
              [Docs for list connector ID command](https://docs.confluent.io/confluent-cli/current/command-reference/connect/cluster/confluent_connect_cluster_list.html)
            </td>
          </tr>
          <tr>
            <td>
              `SCHEMA_REGISTRY_ID`
            </td>
            <td>
              (OPTIONAL) You can monitor your Confluent Schema Registry by specifying the ID here
            </td>
            <td>
              [Docs for list connector ID command](https://docs.confluent.io/confluent-cli/current/command-reference/schema-registry/schema/confluent_schema-registry_schema_list.html)
            </td>
          </tr>
        </tbody>
    </table>


## Step 5: Set up dashboards in New Relic [#dashboard]

Check out this New Relic [Confluent Cloud dashboard](https://onenr.io/02wdnxBDXjE) that uses these metrics:


### Confluent Cloud metrics [#confluent-metrics]

<table>
  <thead>
    <tr>
      <th style={{ width: "200px" }}>
        Name
      </th>
      <th>
        Description
      </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>
        confluent_kafka_server_received_bytes
      </td>
      <td>
        The delta count of bytes of the customer's data received from the network. Each sample is the number of bytes received since the previous data sample. The count is sampled every 60 seconds.
      </td>
    </tr>
    <tr>
      <td>
        confluent_kafka_server_sent_bytes
      </td>
      <td>
        The delta count of bytes of the customer's data sent over the network. Each sample is the number of bytes sent since the previous data point. The count is sampled every 60 seconds.
      </td>
    </tr>
    <tr>
      <td>
        confluent_kafka_server_received_records
      </td>
      <td>
        The delta count of records received. Each sample is the number of records received since the previous data sample. The count is sampled every 60 seconds.
      </td>
    </tr>
    <tr>
      <td>
        confluent_kafka_server_sent_records
      </td>
      <td>
        The delta count of records sent. Each sample is the number of records sent since the previous data point. The count is sampled every 60 seconds.
      </td>
    </tr>
    <tr>
      <td>
        confluent_kafka_server_retained_bytes
      </td>
      <td>
        The current count of bytes retained by the cluster. The count is sampled every 60 seconds.
      </td>
    </tr>
    <tr>
      <td>
        confluent_kafka_server_active_connection_count
      </td>
      <td>
        The count of active authenticated connections.
      </td>
    </tr>
    <tr>
      <td>
        confluent_kafka_server_request_count
      </td>
      <td>
        The delta count of requests received over the network. Each sample is the number of requests received since the previous data point. The count sampled every 60 seconds.
      </td>
    </tr>
    <tr>
      <td>
        confluent_kafka_server_partition_count
      </td>
      <td>
        The number of partitions
      </td>
    </tr>
    <tr>
      <td>
        confluent_kafka_server_successful_authentication_count
      </td>
      <td>
        The delta count of successful authentications. Each sample is the number of successful authentications since the previous data point. The count sampled every 60 seconds.
      </td>
    </tr>
    <tr>
      <td>
        confluent_kafka_server_consumer_lag_offsets
      </td>
      <td>
        The lag between a group member's committed offset and the partition's high watermark. 
      </td>
    </tr>
  </tbody>
</table>


